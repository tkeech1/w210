{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"XC5xNrunHAgl"}},{"cell_type":"code","source":["import os"],"metadata":{"id":"X94cF5hsPcDL","executionInfo":{"status":"ok","timestamp":1710795268081,"user_tz":180,"elapsed":270,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Spark & Pyspark"],"metadata":{"id":"3SkCRdjDeuf7"}},{"cell_type":"code","source":["!apt-get update -qq\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget http://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n","!tar xf spark-3.4.1-bin-hadoop3.tgz\n","!pip install hyperopt mlflow pyngrok findspark xgboost==2.0.3 pyspark==3.4.1 cudf-cu11 --extra-index-url=https://pypi.nvidia.com"],"metadata":{"id":"odlOXixBCH9V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710795319915,"user_tz":180,"elapsed":51434,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}},"outputId":"c29e9446-ff5b-44fb-b0ba-470a529a5be5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-03-18 20:54:48--  http://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n","Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n","Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz [following]\n","--2024-03-18 20:54:48--  https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n","Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 388341449 (370M) [application/x-gzip]\n","Saving to: ‘spark-3.4.1-bin-hadoop3.tgz.1’\n","\n","spark-3.4.1-bin-had 100%[===================>] 370.35M  24.3MB/s    in 16s     \n","\n","2024-03-18 20:55:05 (22.8 MB/s) - ‘spark-3.4.1-bin-hadoop3.tgz.1’ saved [388341449/388341449]\n","\n","Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n","Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (0.2.7)\n","Requirement already satisfied: mlflow in /usr/local/lib/python3.10/dist-packages (2.11.1)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.5)\n","Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n","Requirement already satisfied: xgboost==2.0.3 in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: pyspark==3.4.1 in /usr/local/lib/python3.10/dist-packages (3.4.1)\n","Requirement already satisfied: cudf-cu11 in /usr/local/lib/python3.10/dist-packages (24.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost==2.0.3) (1.24.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost==2.0.3) (1.11.4)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.1) (0.10.9.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.16.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt) (3.2.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt) (0.18.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hyperopt) (4.66.2)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt) (2.2.1)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (8.1.7)\n","Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4)\n","Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.42)\n","Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.0.1)\n","Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (4.25.3)\n","Requirement already satisfied: pytz<2025 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2023.4)\n","Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.31.0)\n","Requirement already satisfied: packaging<24 in /usr/local/lib/python3.10/dist-packages (from mlflow) (23.2)\n","Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (7.0.2)\n","Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4.4)\n","Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.13.1)\n","Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (7.0.0)\n","Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.5)\n","Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.5.3)\n","Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.4)\n","Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.28)\n","Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.2)\n","Requirement already satisfied: pyarrow<16,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (14.0.2)\n","Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.5.2)\n","Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n","Requirement already satisfied: graphene<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.3)\n","Requirement already satisfied: gunicorn<22 in /usr/local/lib/python3.10/dist-packages (from mlflow) (21.2.0)\n","Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.3)\n","Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (5.3.3)\n","Requirement already satisfied: cubinlinker-cu11 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (0.3.0.post1)\n","Requirement already satisfied: cuda-python<12.0a0,>=11.7.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (11.8.3)\n","Requirement already satisfied: cupy-cuda11x>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (13.0.0)\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (2023.6.0)\n","Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (0.58.1)\n","Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (0.2.10)\n","Requirement already satisfied: ptxcompiler-cu11 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (0.8.1.post1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (13.7.1)\n","Requirement already satisfied: rmm-cu11==24.2.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (24.2.0)\n","Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (4.10.0)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.2)\n","Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda11x>=12.0.0->cudf-cu11) (0.8.2)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.0.7)\n","Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.0.1)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (2.1.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\n","Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.3)\n","Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.0)\n","Requirement already satisfied: aniso8601<10,>=8 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (9.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.18.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (2.8.2)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57->cudf-cu11) (0.41.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.3.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu11) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu11) (2.16.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->cudf-cu11) (0.1.2)\n"]}]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n","\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = (\n","    SparkSession.builder\n","    .master('local[*]')\n","    .appName(\"model_experiment\")\n","    .config(\"spark.ui.port\", \"4050\")\n","    .config(\"spark.driver.memory\", \"8g\")\n","    .config(\"spark.executor.memory\", \"16g\")\n","    .getOrCreate()\n",")"],"metadata":{"id":"Fp1DLBYIeuf7","executionInfo":{"status":"ok","timestamp":1710795325213,"user_tz":180,"elapsed":5302,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Paths Configs"],"metadata":{"id":"bPQHL4liHI6T"}},{"cell_type":"code","source":["# path to the this notebook\n","# NOTE: Replace this with your project path if needed\n","PROJECT_PATH = (\n","    \"/content/drive/My Drive/W210\"\n","    if \"google.colab\" in str(get_ipython())\n","    else \".\"\n",")\n","\n","# path to the data folder\n","# NOTE: Replace this with your data path if needed\n","DATA_PATH = f\"{PROJECT_PATH}/data\" if \"google.colab\" in str(get_ipython()) else PROJECT_PATH\n","# NOTE: For colab we use content so it doesn\"t load on google drive storage\n","RAW_DATA_PATH = f\"{PROJECT_PATH}/data\" if \"google.colab\" in str(get_ipython()) else f\"{PROJECT_PATH}/data\""],"metadata":{"id":"AJuHdmtHHLIT","executionInfo":{"status":"ok","timestamp":1710795325213,"user_tz":180,"elapsed":3,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Colab Drive Mount"],"metadata":{"id":"tzvE6EGWHmJn"}},{"cell_type":"code","source":["if \"google.colab\" in str(get_ipython()):\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","os.chdir(PROJECT_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTfRKqC-HpDj","outputId":"3a6b51ae-f343-4c54-d6e9-224b875279b2","executionInfo":{"status":"ok","timestamp":1710795327058,"user_tz":180,"elapsed":1848,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## MLFlow and NGrok"],"metadata":{"id":"Oz0A-DkixmfV"}},{"cell_type":"code","source":["!export MLFLOW_TRACKING_URI=sqlite:///mlruns.db"],"metadata":{"id":"aCx0ka7W17O_","executionInfo":{"status":"ok","timestamp":1710795327058,"user_tz":180,"elapsed":5,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["!nohup mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlruns.db &"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqF-k31zJe1K","executionInfo":{"status":"ok","timestamp":1710795327058,"user_tz":180,"elapsed":4,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}},"outputId":"568864f5-ffd0-4352-8043-00828cba6c7f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["nohup: appending output to 'nohup.out'\n"]}]},{"cell_type":"code","source":["!pgrep mlflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qPKkHHTnx_Aq","executionInfo":{"status":"ok","timestamp":1710795327462,"user_tz":180,"elapsed":407,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}},"outputId":"9ce1ec29-ef9f-47e9-850f-9cbe424651e8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["371436\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok\n","\n","# Terminate open tunnels if exist\n","ngrok.kill()\n","\n","# Setting the authtoken (optional)\n","# Get your authtoken from https://dashboard.ngrok.com/auth\n","from google.colab import userdata\n","NGROK_AUTH_TOKEN = userdata.get('ngrok_token')\n","ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n","\n","# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n","ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n","print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1k9lB4GaB-mN","executionInfo":{"status":"ok","timestamp":1710795328942,"user_tz":180,"elapsed":1483,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}},"outputId":"6338062b-1acb-4e34-a57f-1da11bccd725"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["MLflow Tracking UI: https://ea41-34-75-36-62.ngrok-free.app\n"]}]},{"cell_type":"markdown","source":["## Python Libraries"],"metadata":{"id":"DPLUbOgQHwyj"}},{"cell_type":"code","source":["import itertools\n","\n","from datetime import datetime\n","from dateutil.relativedelta import relativedelta\n","from itertools import chain\n","from typing import Any, Dict, List, Optional, Tuple\n","\n","import matplotlib.pyplot as plt\n","import mlflow\n","import mlflow.spark\n","import numpy as np\n","import pandas as pd\n","import pyspark.sql.functions as F\n","import pyspark.sql.types as T\n","import sklearn.metrics as metrics\n","\n","from hyperopt import hp, fmin, tpe, Trials, space_eval\n","from pyspark.ml import Pipeline\n","from pyspark.ml.classification import GBTClassifier\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.ml.feature import Imputer\n","from pyspark.ml.feature import OneHotEncoder\n","from pyspark.ml.feature import StandardScaler\n","from pyspark.ml.feature import StringIndexer\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","from pyspark.sql import DataFrame\n","from pyspark.sql.window import Window\n","from sklearn.base import BaseEstimator\n","from sklearn.calibration import calibration_curve\n","from sklearn.calibration import IsotonicRegression\n","from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n","from sklearn.utils import check_consistent_length\n","from sklearn.utils import column_or_1d\n","from xgboost.spark import SparkXGBRegressor\n","from tqdm import tqdm"],"metadata":{"id":"r-W5SPtnm-xF","executionInfo":{"status":"ok","timestamp":1710795332946,"user_tz":180,"elapsed":4005,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"nWMmH4MwIC7q"}},{"cell_type":"markdown","source":["# Definitions"],"metadata":{"id":"mxlc3Gw8U5b4"}},{"cell_type":"markdown","source":["## Transformation Pipeline"],"metadata":{"id":"0MjuLpq-BCM-"}},{"cell_type":"code","source":["from pyspark.ml.feature import Imputer\n","from pyspark.ml.feature import OneHotEncoder\n","from pyspark.ml.feature import StandardScaler\n","from pyspark.ml.feature import StringIndexer\n","from pyspark.ml.feature import VectorAssembler\n","\n","\n","def build_transformation_pipeline(\n","    numerical_features: list,\n","    norm_features: list,\n","    sindex_cols: list,\n","    ohe_cols: list,\n","    labelCol: str = \"shortage_flag\",\n","    outputCol: str = \"features\",\n",") -> list:\n","    \"\"\"\n","    Build a model pipeline including all possible feature transformations\n","\n","    :param numerical_features: list of numerical features (to be normalized and imputed)\n","    :param norm_features: list of numerical features to be normalized\n","    :param sindex_cols: list of categorical features to be indexed\n","    :param ohe_cols: list of categorical features to be one-hot encoded\n","    :param labelCol: target label column name\n","    :param outputCol: output feature column name\n","    :return: list of spark pipeline stages\n","    \"\"\"\n","    stages = list()\n","\n","    # create StringIndexer for each text column\n","    if len(sindex_cols) > 0:\n","        indexers = StringIndexer(inputCols=sindex_cols, outputCols=[f\"{c}_index\" for c in sindex_cols])\n","        stages.append(indexers)\n","        icols = indexers.getOutputCols()\n","    else:\n","        icols = list()\n","\n","    # create the one hot encoding\n","    if len(sindex_cols + ohe_cols) > 0:\n","        assembler_inputs = [f\"{c}_ohe\" for c in sindex_cols + ohe_cols]\n","        ohe = OneHotEncoder(inputCols=icols + ohe_cols, outputCols=assembler_inputs)\n","        stages.append(ohe)\n","    else:\n","        assembler_inputs = []\n","\n","    # impute values\n","    if len(numerical_features) > 0:\n","        impute = Imputer(inputCols=numerical_features, outputCols=[f\"{c}_nonnulls\" for c in numerical_features])\n","        stages.append(impute)\n","\n","    # normalize column values\n","    if len(norm_features) > 0:\n","        norm_assemble = VectorAssembler(inputCols=[f\"{c}_nonnulls\" for c in norm_features], outputCol=\"numerical\")\n","        normalization = StandardScaler(inputCol=\"numerical\", outputCol=\"normalized\")\n","        stages += [norm_assemble, normalization]\n","        assembler_inputs += [\"normalized\"]\n","\n","    # assemble the indexed columns into a single feature vector\n","    assembler_inputs += list(set(numerical_features) - set(norm_features))\n","    assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=outputCol)\n","    stages.append(assembler)\n","\n","    return stages"],"metadata":{"id":"sQw5bDbUBBq-","executionInfo":{"status":"ok","timestamp":1710795332946,"user_tz":180,"elapsed":2,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## Cross-Validation"],"metadata":{"id":"Sd__RuURBHkO"}},{"cell_type":"code","source":["from datetime import datetime\n","from dateutil.relativedelta import relativedelta\n","from typing import Optional, List, Tuple, Dict, Any\n","\n","from pyspark import keyword_only\n","from pyspark.ml import Estimator\n","from pyspark.ml.evaluation import Evaluator\n","from pyspark.ml.param import Params, Param, TypeConverters\n","from pyspark.ml.tuning import _ValidatorParams, _CrossValidatorParams\n","from pyspark.ml.tuning import CrossValidator\n","\n","\n","@F.udf(T.IntegerType())\n","def my_rand():\n","    return np.random.randint(1000)\n","\n","\n","def adjust_train_and_test(train_df, test_df):\n","    \"\"\"\n","    Make sure that the train and test data do not share positive cases\n","\n","    :param train_df: pyspark train data frame\n","    :param test_df: pyspark test data frame\n","    :return: train and test data frames adjusted for leakeage\n","    \"\"\"\n","    train_ndc = train_df.filter(F.col(\"shortage_flag\") == 1).groupBy(\"package_ndc\").agg(F.sum(\"shortage_flag\").alias(\"shortage_train\"))\n","    test_ndc = test_df.filter(F.col(\"shortage_flag\") == 1).groupBy(\"package_ndc\").agg(F.sum(\"shortage_flag\").alias(\"shortage_test\"))\n","\n","    join = train_ndc.join(test_ndc, on=\"package_ndc\", how=\"inner\").withColumn(\"rand\", my_rand())\n","    remove_from_train = [\n","        r[0] for r in\n","        join.filter((F.col(\"shortage_test\") > F.col(\"shortage_train\")) | ((F.col(\"shortage_test\") == F.col(\"shortage_train\")) & (F.col(\"rand\") >= 500)))\n","        .select(\"package_ndc\")\n","        .collect()\n","    ]\n","\n","    remove_from_test = [\n","        r[0] for r in\n","        join.filter(~F.col(\"package_ndc\").isin(remove_from_train))\n","        .select(\"package_ndc\")\n","        .collect()\n","    ]\n","\n","    return (\n","        train_df.filter(~F.col(\"package_ndc\").isin(remove_from_train)).cache(),\n","        test_df.filter(~F.col(\"package_ndc\").isin(remove_from_test)).cache(),\n","    )\n","\n","\n","class _TimeSeriesCrossValidatorParams(_ValidatorParams):\n","    \"\"\"\n","    Parameters used to define the TimeSeriesCrossValidator.\n","    \"\"\"\n","    numFolds = Param(\n","        Params._dummy(),\n","        \"numFolds\",\n","        \"number of folds for cross validation\",\n","        typeConverter=TypeConverters.toInt,\n","    )\n","\n","    foldCol = Param(\n","        Params._dummy(),\n","        \"foldCol\",\n","        \"Param for the column name of user \"\n","        + \"specified fold number. Once this is specified, :py:class:`CrossValidator` \"\n","        + \"won't do random k-fold split. Note that this column should be integer type \"\n","        + \"with range [0, numFolds) and Spark will throw exception on out-of-range \"\n","        + \"fold numbers.\",\n","        typeConverter=TypeConverters.toString,\n","    )\n","\n","    dateCol = Param(\n","        Params._dummy(),\n","        \"dateCol\",\n","        \"\",\n","        typeConverter=TypeConverters.toString,\n","    )\n","\n","    cvStartDate = Param(\n","        Params._dummy(),\n","        \"cvStartDate\",\n","        \"\",\n","        typeConverter=TypeConverters.toString,\n","    )\n","\n","    cvEndDate = Param(\n","        Params._dummy(),\n","        \"cvEndDate\",\n","        \"\",\n","        typeConverter=TypeConverters.toString,\n","    )\n","\n","    cvValSize = Param(\n","        Params._dummy(),\n","        \"cvValSize\",\n","        \"\",\n","        typeConverter=TypeConverters.toString\n","    )\n","\n","    cvValStep = Param(\n","        Params._dummy(),\n","        \"cvValStep\",\n","        \"\",\n","        typeConverter=TypeConverters.toString\n","    )\n","\n","    def __init__(self, *args: Any):\n","        super(_CrossValidatorParams, self).__init__(*args)\n","        self._setDefault(dateCol=\"cd_day\")\n","        self._setDefault(cvStartDate=\"2022-02-01\")\n","        self._setDefault(cvEndDate=\"2022-03-15\")\n","        self._setDefault(cvValSize=\"weeks=2\")\n","        self._setDefault(cvValStep=\"weeks=2\")\n","        self._setDefault(numFolds=3, foldCol=\"\")\n","\n","\n","class TimeSeriesCrossValidator(_TimeSeriesCrossValidatorParams, CrossValidator):\n","    \"\"\"\n","    The TimeSeriesCrossValidator class runs sequential time series cross validation\n","    within the spark ml framework given configurations about how to run the\n","    cross validation.\n","    \"\"\"\n","    _input_kwargs: Dict[str, Any]\n","\n","    @keyword_only\n","    def __init__(\n","        self,\n","        *,\n","        estimator: Optional[Estimator] = None,\n","        estimatorParamMaps: Optional[List[\"ParamMap\"]] = None,\n","        evaluator: Optional[Evaluator] = None,\n","        numFolds: int = 3,\n","        seed: Optional[int] = None,\n","        parallelism: int = 1,\n","        collectSubModels: bool = False,\n","        foldCol: str = \"\",\n","        dateCol: str = \"\",\n","        cvStartDate: str = \"\",\n","        cvEndDate: str = \"\",\n","        cvValSize: str = \"\",\n","        cvValStep: str = \"\",\n","    ) -> None:\n","        \"\"\"\n","        Initialize the time series cross validator\n","\n","        :param estimator: estimator object to train\n","        :param estimatorParamMaps: parameter grid to run over cross validation\n","        :param evaluator: model evaluation\n","        :param numFolds: number of folds to create (NOT USED)\n","        :param seed: random seed\n","        :param parallelism: number of parallel jobs\n","        :param collectSubModels: whether to collect sub models\n","        :param foldCol: column name of user specified fold number\n","        :param dateCol: column name of date\n","        :param cvStartDate: start date of cross validation\n","        :param cvEndDate: end date of cross validation\n","        :param cvValSize: size of validation set (weeks=X, months=X, years=X)\n","        \"\"\"\n","        super(TimeSeriesCrossValidator, self).__init__()\n","        self._setDefault(parallelism=1)\n","        self._setDefault(dateCol=\"cd_day\")\n","        self._setDefault(cvStartDate=\"20220201\")\n","        self._setDefault(cvEndDate=\"20220315\")\n","        self._setDefault(cvValSize=\"weeks=2\")\n","        self._setDefault(cvValStep=\"weeks=2\")\n","        self._setDefault(numFolds=3)\n","        kwargs = self._input_kwargs\n","        self._set(**kwargs)\n","\n","    def _kFold(self, dataset: DataFrame) -> List[Tuple[DataFrame, DataFrame]]:\n","        \"\"\"\n","        Given a dataset, creates the train and test splits for each fold\n","\n","        :param dataset: dataset to split\n","        :return: list of train and test splits\n","        \"\"\"\n","        # get the parameters\n","        dateCol = self.getOrDefault(self.dateCol)\n","        cvStartDate = self.getOrDefault(self.cvStartDate)\n","        cvEndDate = self.getOrDefault(self.cvEndDate)\n","        cvValSize = self.getOrDefault(self.cvValSize)\n","        cvValStep = self.getOrDefault(self.cvValStep)\n","        numFolds = self.getOrDefault(self.numFolds)\n","\n","        # convert cvValSize type\n","        k = cvValSize.split(\"=\")[0]\n","        v = int(cvValSize.split(\"=\")[1])\n","        cvValSize = relativedelta(**{k: v})\n","\n","        # convert cvValStep type\n","        k = cvValStep.split(\"=\")[0]\n","        v = int(cvValStep.split(\"=\")[1])\n","        cvValStep = relativedelta(**{k: v})\n","\n","        # calculate the list of dates to be used\n","        date_format = \"%Y%m%d\"\n","        start_date = datetime.strptime(cvStartDate, date_format)\n","        end_date = datetime.strptime(cvEndDate, date_format)\n","        validation_list = []\n","        count = 0\n","        while start_date < end_date:\n","            validation_list.append(\n","                (\n","                    start_date.strftime(date_format),\n","                    (start_date + cvValSize).strftime(date_format)\n","                )\n","            )\n","            start_date += cvValStep\n","            count += 1\n","            if count == numFolds:\n","                break\n","\n","        # Do k-fold split.\n","        datasets = []\n","        for i, (train_end_time, validation_end_time) in enumerate(validation_list):\n","            # split the data between train and test\n","            train = dataset.filter(F.col(dateCol) < train_end_time)\n","            validation = dataset.filter(\n","                (F.col(dateCol) >= train_end_time) & (F.col(dateCol) < validation_end_time)\n","            )\n","\n","            # adjust for leakage\n","            train, validation = adjust_train_and_test(train, validation)\n","            train = train.cache()\n","            validation = validation.cache()\n","\n","            # save data frames\n","            datasets.append((train, validation))\n","\n","        return datasets"],"metadata":{"id":"veTtgZUPBHSe","executionInfo":{"status":"ok","timestamp":1710795335010,"user_tz":180,"elapsed":2065,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## Metrics Calculations"],"metadata":{"id":"fpbfNxc8A-8e"}},{"cell_type":"code","source":["import pyspark.sql.functions as F\n","\n","from pyspark.ml import Pipeline\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","from pyspark.sql import DataFrame\n","\n","\n","def get_all_metrics(model: Pipeline, test_data: DataFrame, labelCol: str = \"shortage_flag\") -> dict:\n","    \"\"\"\n","    Given a model pipeline and data to be evaluated against, get a dictionary\n","    with binary classification metrics for that trained pipeline\n","\n","    :param model: spark pipeline modelo\n","    :param test_data: spark data frame with test data\n","    :param labelCol: label column name\n","    :return: dictionary with metrics (precision, recall, f1, auc)\n","    \"\"\"\n","    metrics_dict = dict()\n","\n","    # create the prediction data frame\n","    prediction = model.transform(test_data).withColumn(labelCol, F.col(labelCol).cast(\"double\"))\n","\n","    # get the multi-class metrics\n","    mmetrics = MulticlassMetrics(prediction.select(\"prediction\", labelCol).rdd)\n","    metrics_dict[\"precision_0\"] = mmetrics.precision(0)\n","    metrics_dict[\"precision_1\"] = mmetrics.precision(1)\n","    metrics_dict[\"recall_0\"] = mmetrics.recall(0)\n","    metrics_dict[\"recall_1\"] = mmetrics.recall(1)\n","    metrics_dict[\"f1_0\"] = mmetrics.fMeasure(0.0)\n","    metrics_dict[\"f1_1\"] = mmetrics.fMeasure(1.0)\n","\n","    # get the area under the curve\n","    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=labelCol)\n","    metrics_dict[\"auc\"] = evaluator.evaluate(prediction, {evaluator.metricName: \"areaUnderROC\"})\n","\n","    return metrics_dict"],"metadata":{"id":"x13SbFHPYN3T","executionInfo":{"status":"ok","timestamp":1710795335010,"user_tz":180,"elapsed":3,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## Extract Probabilities"],"metadata":{"id":"DlxjNFcnR9Sf"}},{"cell_type":"code","source":["import pyspark.sql.functions as F\n","import pyspark.sql.types as T\n","\n","\n","def extract_prob(v):\n","    try:\n","        return float(v[1])\n","    except ValueError:\n","        return None\n","extract_prob_udf = F.udf(extract_prob, T.DoubleType())"],"metadata":{"id":"vpNpG48AR8p_","executionInfo":{"status":"ok","timestamp":1710795335010,"user_tz":180,"elapsed":2,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## Calibration"],"metadata":{"id":"blGJqK4qI6eP"}},{"cell_type":"code","source":["from typing import Dict, List, Tuple, Optional\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import sklearn.metrics as metrics\n","\n","from sklearn.base import BaseEstimator\n","from sklearn.utils import check_consistent_length\n","from sklearn.utils import column_or_1d\n","from sklearn.calibration import calibration_curve\n","from sklearn.calibration import IsotonicRegression\n","from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n","\n","\n","def compute_calibration_error(\n","    y_true: np.ndarray,\n","    y_prob: np.ndarray,\n","    n_bins: int=15,\n","    round_digits: int=4\n",") -> float:\n","    \"\"\"\n","    Computes the calibration error for binary classification via binning\n","    data points into the specified number of bins. Samples with similar\n","    ``y_prob`` will be grouped into the same bin. The bin boundary is\n","    determined by having similar number of samples within each bin.\n","\n","    Parameters\n","    ----------\n","    y_true : 1d ndarray\n","        Binary true targets.\n","\n","    y_prob : 1d ndarray\n","        Raw probability/score of the positive class.\n","\n","    n_bins : int, default 15\n","        A bigger bin number requires more data. In general,\n","        the larger the bin size, the closer the calibration error\n","        will be to the true calibration error.\n","\n","    round_digits : int, default 4\n","        Round the calibration error metric.\n","\n","    Returns\n","    -------\n","    calibration_error : float\n","        RMSE between the average positive label and predicted probability\n","        within each bin.\n","    \"\"\"\n","    y_true = column_or_1d(y_true)\n","    y_prob = column_or_1d(y_prob)\n","    check_consistent_length(y_true, y_prob)\n","\n","    binned_y_true, binned_y_prob = create_binned_data(y_true, y_prob, n_bins)\n","\n","    # looping shouldn't be a source of bottleneck as n_bins should be a small number.\n","    bin_errors = 0.0\n","    for bin_y_true, bin_y_prob in zip(binned_y_true, binned_y_prob):\n","        avg_y_true = np.mean(bin_y_true)\n","        avg_y_score = np.mean(bin_y_prob)\n","        bin_error = (avg_y_score - avg_y_true) ** 2\n","        bin_errors += bin_error * len(bin_y_true)\n","\n","    calibration_error = np.sqrt(bin_errors / len(y_true))\n","    return round(calibration_error, round_digits)\n","\n","\n","def create_binned_data(\n","    y_true: np.ndarray,\n","    y_prob: np.ndarray,\n","    n_bins: int\n",") -> Tuple[List[np.ndarray], List[np.ndarray]]:\n","    \"\"\"\n","    Bin ``y_true`` and ``y_prob`` by distribution of the data.\n","    i.e. each bin will contain approximately an equal number of\n","    data points. Bins are sorted based on ascending order of ``y_prob``.\n","\n","    Parameters\n","    ----------\n","    y_true : 1d ndarray\n","        Binary true targets.\n","\n","    y_prob : 1d ndarray\n","        Raw probability/score of the positive class.\n","\n","    n_bins : int, default 15\n","        A bigger bin number requires more data.\n","\n","    Returns\n","    -------\n","    binned_y_true/binned_y_prob : 1d ndarray\n","        Each element in the list stores the data for that bin.\n","    \"\"\"\n","    sorted_indices = np.argsort(y_prob)\n","    sorted_y_true = y_true[sorted_indices]\n","    sorted_y_prob = y_prob[sorted_indices]\n","    binned_y_true = np.array_split(sorted_y_true, n_bins)\n","    binned_y_prob = np.array_split(sorted_y_prob, n_bins)\n","    return binned_y_true, binned_y_prob\n","\n","\n","def get_bin_boundaries(binned_y_prob: List[np.ndarray]) -> np.ndarray:\n","    \"\"\"\n","    Given ``binned_y_prob`` from ``create_binned_data`` get the\n","    boundaries for each bin.\n","\n","    Parameters\n","    ----------\n","    binned_y_prob : list\n","        Each element in the list stores the data for that bin.\n","\n","    Returns\n","    -------\n","    bins : 1d ndarray\n","        Boundaries for each bin.\n","    \"\"\"\n","    bins = []\n","    for i in range(len(binned_y_prob) - 1):\n","        last_prob = binned_y_prob[i][-1]\n","        next_first_prob = binned_y_prob[i + 1][0]\n","        bins.append((last_prob + next_first_prob) / 2.0)\n","\n","    bins.append(1.0)\n","    return np.array(bins)\n","\n","\n","def compute_binary_score(\n","    y_true: np.ndarray,\n","    y_prob: np.ndarray,\n","    round_digits: int=4\n",") -> Dict[str, float]:\n","    \"\"\"\n","    Compute various evaluation metrics for binary classification.\n","    Including auc, precision, recall, f1, log loss, brier score. The\n","    threshold for precision and recall numbers are based on the one\n","    that gives the best f1 score.\n","\n","    Parameters\n","    ----------\n","    y_true : 1d ndarray\n","        Binary true targets.\n","\n","    y_prob : 1d ndarray\n","        Raw probability/score of the positive class.\n","\n","    round_digits : int, default 4\n","        Round the evaluation metric.\n","\n","    Returns\n","    -------\n","    metrics_dict : dict\n","        Metrics are stored in key value pair. ::\n","\n","        {\n","            'auc': 0.82,\n","            'precision': 0.56,\n","            'recall': 0.61,\n","            'f1': 0.59,\n","            'log_loss': 0.42,\n","            'brier': 0.12\n","        }\n","    \"\"\"\n","    auc = round(metrics.roc_auc_score(y_true, y_prob), round_digits)\n","    log_loss = round(metrics.log_loss(y_true, y_prob), round_digits)\n","    brier_score = round(metrics.brier_score_loss(y_true, y_prob), round_digits)\n","\n","    precision, recall, threshold = metrics.precision_recall_curve(y_true, y_prob)\n","    f1 = 2 * (precision * recall) / (precision + recall)\n","\n","    mask = ~np.isnan(f1)\n","    f1 = f1[mask]\n","    precision = precision[mask]\n","    recall = recall[mask]\n","\n","    best_index = np.argmax(f1)\n","    precision = round(precision[best_index], round_digits)\n","    recall = round(recall[best_index], round_digits)\n","    f1 = round(f1[best_index], round_digits)\n","    return {\n","        'auc': auc,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1,\n","        'log_loss': log_loss,\n","        'brier': brier_score\n","    }\n","\n","\n","def compute_calibration_summary(\n","    eval_dict: Dict[str, pd.DataFrame],\n","    label_col: str='label',\n","    score_col: str='score',\n","    n_bins: int=15,\n","    strategy: str='quantile',\n","    round_digits: int=4,\n","    show: bool=True,\n","    save_plot_path: Optional[str]=None\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Plots the calibration curve and computes the summary statistics for the model.\n","\n","    Parameters\n","    ----------\n","    eval_dict : dict\n","        We can evaluate multiple calibration model's performance in one go. The key\n","        is the model name used to distinguish different calibration model, the value\n","        is the dataframe that stores the binary true targets and the predicted score\n","        for the positive class.\n","\n","    label_col : str\n","        Column name for the dataframe in ``eval_dict`` that stores the binary true targets.\n","\n","    score_col : str\n","        Column name for the dataframe in ``eval_dict`` that stores the predicted score.\n","\n","    n_bins : int, default 15\n","        Number of bins to discretize the calibration curve plot and calibration error statistics.\n","        A bigger number requires more data, but will be closer to the true calibration error.\n","\n","    strategy : {'uniform', 'quantile'}, default 'quantile'\n","        Strategy used to define the boundary of the bins.\n","\n","        - uniform: The bins have identical widths.\n","        - quantile: The bins have the same number of samples and depend on the predicted score.\n","\n","    round_digits : default 4\n","        Round the evaluation metric.\n","\n","    show : bool, default True\n","        Whether to show the plots on the console or jupyter notebook.\n","\n","    save_plot_path : str, default None\n","        Path where we'll store the calibration plot. None means it will not save the plot.\n","\n","    Returns\n","    -------\n","    df_metrics : pd.DataFrame\n","        Corresponding metrics for all the input dataframe.\n","    \"\"\"\n","\n","    fig, (ax1, ax2) = plt.subplots(2)\n","\n","    # estimator_metrics stores list of dict, e.g.\n","    # [{'auc': 0.776, 'name': 'xgb'}]\n","    estimator_metrics = []\n","    for name, df_eval in eval_dict.items():\n","        prob_true, prob_pred = calibration_curve(\n","            df_eval[label_col],\n","            df_eval[score_col],\n","            n_bins=n_bins,\n","            strategy=strategy\n","        )\n","\n","        calibration_error = compute_calibration_error(\n","            df_eval[label_col], df_eval[score_col], n_bins, round_digits\n","        )\n","        metrics_dict = compute_binary_score(df_eval[label_col], df_eval[score_col], round_digits)\n","        metrics_dict['calibration_error'] = calibration_error\n","        metrics_dict['name'] = name\n","        estimator_metrics.append(metrics_dict)\n","\n","        ax1.plot(prob_pred, prob_true, 's-', label=name)\n","        ax2.hist(df_eval[score_col], range=(0, 1), bins=n_bins, label=name, histtype='step', lw=2)\n","\n","    ax1.plot([0, 1], [0, 1], 'k:', label='perfect')\n","\n","    ax1.set_xlabel('Fraction of positives (Predicted)')\n","    ax1.set_ylabel('Fraction of positives (Actual)')\n","    ax1.set_ylim([-0.05, 1.05])\n","    ax1.legend(loc='upper left', ncol=2)\n","    ax1.set_title('Calibration Plots (Reliability Curve)')\n","\n","    ax2.set_xlabel('Predicted scores')\n","    ax2.set_ylabel('Count')\n","    ax2.set_title('Histogram of Predicted Scores')\n","    ax2.legend(loc='upper right', ncol=2)\n","\n","    plt.tight_layout()\n","    if show:\n","        plt.show()\n","\n","    if save_plot_path is not None:\n","        save_dir = os.path.dirname(save_plot_path)\n","        if save_dir:\n","            os.makedirs(save_dir, exist_ok=True)\n","\n","        fig.savefig(save_plot_path, dpi=300, bbox_inches='tight')\n","\n","    plt.close(fig)\n","\n","    df_metrics = pd.DataFrame(estimator_metrics)\n","    return df_metrics\n","\n","\n","\n","class HistogramCalibrator(BaseEstimator):\n","    \"\"\"\n","    Bins the data based on equal size interval (each bin contains approximately\n","    equal size of samples).\n","\n","    Parameters\n","    ----------\n","    n_bins : int, default 15\n","        A bigger bin number requires more data. In general,\n","        the larger the bin size, the closer the calibration error\n","        will be to the true calibration error.\n","\n","    Attributes\n","    ----------\n","    bins_ : 1d ndarray\n","        Boundaries for each bin.\n","\n","    bins_score_ : 1d ndarray\n","        Calibration score for each bin.\n","    \"\"\"\n","\n","    def __init__(self, n_bins: int=15):\n","        self.n_bins = n_bins\n","\n","    def fit(self, y_prob: np.ndarray, y_true: np.ndarray):\n","        \"\"\"\n","        Learns the bin boundaries and calibration score for each bin.\n","\n","        Parameters\n","        ----------\n","        y_prob : 1d ndarray\n","            Raw probability/score of the positive class.\n","\n","        y_true : 1d ndarray\n","            Binary true targets.\n","\n","        Returns\n","        -------\n","        self\n","        \"\"\"\n","        binned_y_true, binned_y_prob = create_binned_data(y_true, y_prob, self.n_bins)\n","        self.bins_ = get_bin_boundaries(binned_y_prob)\n","        self.bins_score_ = np.array([np.mean(value) for value in binned_y_true])\n","        return self\n","\n","    def predict(self, y_prob: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Predicts the calibrated probability.\n","\n","        Parameters\n","        ----------\n","        y_prob : 1d ndarray\n","            Raw probability/score of the positive class.\n","\n","        Returns\n","        -------\n","        y_calibrated_prob : 1d ndarray\n","            Calibrated probability.\n","        \"\"\"\n","        indices = np.searchsorted(self.bins_, y_prob)\n","        return self.bins_score_[indices]\n","\n","\n","class PlattCalibrator(BaseEstimator):\n","    \"\"\"\n","    Boils down to applying a Logistic Regression.\n","\n","    Parameters\n","    ----------\n","    log_odds : bool, default True\n","        Logistic Regression assumes a linear relationship between its input\n","        and the log-odds of the class probabilities. Converting the probability\n","        to log-odds scale typically improves performance.\n","\n","    Attributes\n","    ----------\n","    coef_ : ndarray of shape (1,)\n","        Binary logistic regression's coefficient.\n","\n","    intercept_ : ndarray of shape (1,)\n","        Binary logistic regression's intercept.\n","    \"\"\"\n","\n","    def __init__(self, log_odds: bool=True):\n","        self.log_odds = log_odds\n","\n","    def fit(self, y_prob: np.ndarray, y_true: np.ndarray):\n","        \"\"\"\n","        Learns the logistic regression weights.\n","\n","        Parameters\n","        ----------\n","        y_prob : 1d ndarray\n","            Raw probability/score of the positive class.\n","\n","        y_true : 1d ndarray\n","            Binary true targets.\n","\n","        Returns\n","        -------\n","        self\n","        \"\"\"\n","        self.fit_predict(y_prob, y_true)\n","        return self\n","\n","    @staticmethod\n","    def _convert_to_log_odds(y_prob: np.ndarray) -> np.ndarray:\n","        eps = 1e-12\n","        y_prob = np.clip(y_prob, eps, 1 - eps)\n","        y_prob = np.log(y_prob / (1 - y_prob))\n","        return y_prob\n","\n","    def predict(self, y_prob: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Predicts the calibrated probability.\n","\n","        Parameters\n","        ----------\n","        y_prob : 1d ndarray\n","            Raw probability/score of the positive class.\n","\n","        Returns\n","        -------\n","        y_calibrated_prob : 1d ndarray\n","            Calibrated probability.\n","        \"\"\"\n","        if self.log_odds:\n","            y_prob = self._convert_to_log_odds(y_prob)\n","\n","        output = self._transform(y_prob)\n","        return output\n","\n","    def _transform(self, y_prob: np.ndarray) -> np.ndarray:\n","        output = y_prob * self.coef_[0] + self.intercept_\n","        output = 1 / (1 + np.exp(-output))\n","        return output\n","\n","    def fit_predict(self, y_prob: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Chain the .fit and .predict step together.\n","\n","        Parameters\n","        ----------\n","        y_prob : 1d ndarray\n","            Raw probability/score of the positive class.\n","\n","        y_true : 1d ndarray\n","            Binary true targets.\n","\n","        Returns\n","        -------\n","        y_calibrated_prob : 1d ndarray\n","            Calibrated probability.\n","        \"\"\"\n","        if self.log_odds:\n","            y_prob = self._convert_to_log_odds(y_prob)\n","\n","        # the class expects 2d ndarray as input features\n","        logistic = SklearnLogisticRegression(C=1e10, solver='lbfgs')\n","        logistic.fit(y_prob.reshape(-1, 1), y_true)\n","        self.coef_ = logistic.coef_[0]\n","        self.intercept_ = logistic.intercept_\n","\n","        y_calibrated_prob = self._transform(y_prob)\n","        return y_calibrated_prob\n","\n","\n","class PlattHistogramCalibrator(PlattCalibrator):\n","    \"\"\"\n","    Boils down to first applying a Logistic Regression then perform\n","    histogram binning.\n","\n","    Parameters\n","    ----------\n","    log_odds : bool, default True\n","        Logistic Regression assumes a linear relationship between its input\n","        and the log-odds of the class probabilities. Converting the probability\n","        to log-odds scale typically improves performance.\n","\n","    n_bins : int, default 15\n","        A bigger bin number requires more data. In general,\n","        the larger the bin size, the closer the calibration error\n","        will be to the true calibration error.\n","\n","    Attributes\n","    ----------\n","    coef_ : ndarray of shape (1,)\n","        Binary logistic regresion's coefficient.\n","\n","    intercept_ : ndarray of shape (1,)\n","        Binary logistic regression's intercept.\n","\n","    bins_ : 1d ndarray\n","        Boundaries for each bin.\n","\n","    bins_score_ : 1d ndarray\n","        Calibration score for each bin.\n","    \"\"\"\n","\n","    def __init__(self, log_odds: bool=True, n_bins: int=15):\n","        super().__init__(log_odds)\n","        self.n_bins = n_bins\n","\n","    def fit(self, y_prob: np.ndarray, y_true: np.ndarray):\n","        \"\"\"\n","        Learns the logistic regression weights and the\n","        bin boundaries and calibration score for each bin.\n","\n","        Parameters\n","        ----------\n","        y_prob : 1d ndarray\n","            Raw probability/score of the positive class.\n","\n","        y_true : 1d ndarray\n","            Binary true targets.\n","\n","        Returns\n","        -------\n","        self\n","        \"\"\"\n","        y_prob_platt = super().fit_predict(y_prob, y_true)\n","        binned_y_true, binned_y_prob = create_binned_data(y_true, y_prob_platt, self.n_bins)\n","        self.bins_ = get_bin_boundaries(binned_y_prob)\n","        self.bins_score_ = np.array([np.mean(value) for value in binned_y_prob])\n","        return self\n","\n","    def predict(self, y_prob: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Predicts the calibrated probability.\n","\n","        Parameters\n","        ----------\n","        y_prob : 1d ndarray\n","            Raw probability/score of the positive class.\n","\n","        Returns\n","        -------\n","        y_calibrated_prob : 1d ndarray\n","            Calibrated probability.\n","        \"\"\"\n","        y_prob_platt = super().predict(y_prob)\n","        indices = np.searchsorted(self.bins_, y_prob_platt)\n","        return self.bins_score_[indices]"],"metadata":{"id":"JgqZntjTI6Mv","executionInfo":{"status":"ok","timestamp":1710795335011,"user_tz":180,"elapsed":3,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"geMuPypiBOWv"}},{"cell_type":"markdown","source":["# Configurations"],"metadata":{"id":"ISHbM4wVn1U4"}},{"cell_type":"code","source":["EXPERIMENT_TAG = \"evaluate\"\n","DATASET_TARGET = \"24M\"\n","WEEKS_AHEAD = 4\n","NUM_EVALS = 15\n","\n","FEATURES = [\n","    \"recalls\",\n","    \"inspections\",\n","    \"in_vai\",\n","    \"in_oai\",\n","    \"compliances\",\n","    \"in_injunction\",\n","    # \"in_seizure\",\n","    \"adverse_events\",\n","    \"in_labeler_shortage\",\n","    # \"pct_shortage\",\n","    \"age\",\n","    \"shortage_indicator\",\n","    \"dosage_implantable\",\n","    \"dosage_inhalable\",\n","    \"dosage_injectable\",\n","    \"dosage_liquid\",\n","    \"dosage_miscellaneous\",\n","    \"dosage_ophthalmic\",\n","    \"dosage_oral_care\",\n","    \"dosage_oral_solid\",\n","    \"dosage_rectal\",\n","    \"dosage_topical\",\n","    \"dosage_transdermal\",\n","    \"dosage_vaginal\",\n","    \"mc_anda\",\n","    \"mc_bla\",\n","    \"mc_bulk_ingredient\",\n","    \"mc_bulk_ingredient_for_human_prescription_compounding\",\n","    \"mc_cosmetic\",\n","    \"mc_drug_for_further_processing\",\n","    \"mc_emergency_use_authorization\",\n","    \"mc_export_only\",\n","    \"mc_nda\",\n","    \"mc_nda_authorized_generic\",\n","    \"mc_otc_monograph_drug\",\n","    \"mc_otc_monograph_final\",\n","    \"mc_otc_monograph_not_final\",\n","    \"mc_unapproved_drug_for_use_in_drug_shortage\",\n","    \"mc_unapproved_drug_other\",\n","    \"mc_unapproved_homeopathic\",\n","    \"mc_unapproved_medical_gas\",\n","    \"route_auricularotic\",\n","    \"route_enteral\",\n","    \"route_extracorporeal\",\n","    \"route_infiltration\",\n","    \"route_intraarticular\",\n","    \"route_intracardiac\",\n","    \"route_intracaudal\",\n","    \"route_intracavernous\",\n","    \"route_intracavitary\",\n","    \"route_intradermal\",\n","    \"route_intraluminal\",\n","    \"route_intralymphatic\",\n","    \"route_intramedullary\",\n","    \"route_intrameningeal\",\n","    \"route_intraperitoneal\",\n","    \"route_intrapleural\",\n","    \"route_intrasinal\",\n","    \"route_intraspinal\",\n","    \"route_intrasynovial\",\n","    \"route_intratympanic\",\n","    \"route_intrauterine\",\n","    \"route_intravascular\",\n","    \"route_intraventricular\",\n","    \"route_intravesical\",\n","    \"route_mucosal\",\n","    \"route_ophthalmic\",\n","    \"route_parenteral\",\n","    \"route_perineural\",\n","    \"route_respiratory\",\n","    \"route_topical\",\n","    \"route_ureteral\",\n","    \"route_urethral\",\n","    \"route_vaginal\",\n","]\n","STRING_INDEX = []\n","OHE_FEATURES = []\n","NORM_FEATURES =  [\n","    \"recalls\",\n","    \"inspections\",\n","    \"compliances\",\n","    \"adverse_events\",\n","    \"age\",\n","]\n","NUMERICAL_FEATURES = list(set(FEATURES) - set(STRING_INDEX) - set(OHE_FEATURES))\n","\n","MIN_DATE = \"20220601\"\n","if DATASET_TARGET == \"3M\":\n","    START_CV_DATE = \"20220201\"\n","    END_CV_DATE = \"20220315\"\n","    END_TEST_DATE = \"20220401\"\n","    CV_SET_SIZE = \"weeks=2\"\n","    CV_SET_STEP = \"weeks=2\"\n","    NUM_FOLDS = 3\n","elif DATASET_TARGET == \"12M\":\n","    START_CV_DATE = \"20220801\"\n","    END_CV_DATE = \"20230101\"\n","    END_TEST_DATE = \"20230201\"\n","    CV_SET_SIZE = \"months=1\"\n","    CV_SET_STEP = \"months=1\"\n","    NUM_FOLDS = 5\n","elif DATASET_TARGET == \"24M\":\n","    START_CV_DATE = \"20230201\"\n","    END_CV_DATE = \"20230801\"\n","    END_TEST_DATE = \"20240101\"\n","    CV_SET_SIZE = \"months=4\"\n","    CV_SET_STEP = \"months=1\"\n","    NUM_FOLDS = 3\n","\n","ALGORITHMS = [\n","    LogisticRegression,\n","    # RandomForestClassifier,\n","    # GBTClassifier,\n","    # SparkXGBRegressor,\n","]\n","\n","GRID_SPACE = {\n","    LogisticRegression: {\n","        \"maxIter\": hp.qloguniform(\"maxIter\", np.log(10), np.log(1000),10),\n","        \"regParam\": hp.loguniform(\"regParam\", np.log(1e-07), np.log(1e-04)),\n","        \"elasticNetParam\": hp.uniform(\"elasticNetParam\", 0, 1),\n","        # \"tol\": [1e-4, 1e-5, 1e-6, 1e-7, 1e-8],\n","        # \"aggregationDepth\": [2, 3],\n","        \"labelCol\": \"shortage_flag\",\n","    },\n","    RandomForestClassifier: {\n","        \"numTrees\": hp.quniform('numTrees', 10, 100, 10),\n","        \"featureSubsetStrategy\":  hp.choice(\"featureSubsetStrategy\" ,['auto', 'all']),\n","        \"impurity\": hp.choice(\"impurity\", ['entropy', 'gini']),\n","        \"maxBins\": hp.choice(\"maxBins\", [32, 64, 128]),\n","        \"maxDepth\": hp.quniform('maxDepth', 7, 30, 1),\n","        \"labelCol\": \"shortage_flag\",\n","        \"seed\": hp.choice(\"seed\", [42]),\n","        # \"bootstrap\": [True, False],\n","        # \"checkpointInterval\": [10, 20],\n","        # \"minInfoGain\": [0.0, 0.001, 0.0001],\n","        # \"minInstancesPerNode\": [1, 5],\n","        # \"subsamplingRate\": [0.7, 1.0],\n","    },\n","    GBTClassifier: {\n","        'maxDepth': hp.quniform('maxDepth', 7, 30, 1),\n","        \"featureSubsetStrategy\":  hp.choice(\"featureSubsetStrategy\" ,['auto', 'all']),\n","        'maxIter': hp.choice('maxIter', [10, 20, 30]),\n","        'stepSize': hp.loguniform('stepSize', -5, 0),\n","        # 'minInstancesPerNode': hp.quniform('minInstancesPerNode', 1, 50, 1),\n","        # 'subsamplingRate': hp.quniform('subsamplingRate', .01, 1, .05),\n","        \"seed\": hp.choice(\"seed\", [42]),\n","        \"labelCol\": \"shortage_flag\",\n","\n","    },\n","    SparkXGBRegressor: {\n","        \"eta\": hp.loguniform(\"eta\", np.log(1e-02), np.log(1)),\n","        \"gamma\": hp.loguniform(\"gamma\", np.log(1e-07), np.log(1e-04)),\n","        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.8, 1),\n","        \"lambda\": hp.loguniform(\"lambda\", np.log(1e-07), np.log(1e-01)),\n","        \"alpha\": hp.loguniform(\"alpha\", np.log(1e-07), np.log(1e-01)),\n","        \"max_depth\": hp.quniform(\"max_depth\", 5, 30, 1),\n","        \"seed\": hp.choice(\"seed\", [42]),\n","        \"labelCol\": \"shortage_flag\",\n","        \"num_workers\": hp.choice(\"num_workers\", [4]),\n","        \"device\": hp.choice(\"device\", [\"cuda\"]),\n","    },\n","}\n","\n","# check that the parameters are correctly setup\n","if EXPERIMENT_TAG not in [\"test\", \"evaluate\", \"final\"]:\n","    del EXPERIMENT_TAG\n","    raise ValueError\n","if DATASET_TARGET not in [\"3M\", \"12M\", \"24M\"]:\n","    del DATASET_TARGET\n","    raise ValueError\n","if not set(STRING_INDEX).issubset(set(FEATURES)):\n","    del STRING_INDEX, FEATURES\n","    raise ValueError\n","if not set(OHE_FEATURES).issubset(set(FEATURES)):\n","    del OHE_FEATURES, FEATURES\n","    raise ValueError\n","if not set(NORM_FEATURES).issubset(set(NUMERICAL_FEATURES)):\n","    del NORM_FEATURES, NUMERICAL_FEATURES\n","    raise ValueError\n","if not set(ALGORITHMS).issubset(set(GRID_SPACE.keys())):\n","    del GRID_SPACE, ALGORITHMS\n","    raise ValueError"],"metadata":{"id":"YNKaykHUn06m","executionInfo":{"status":"ok","timestamp":1710795335011,"user_tz":180,"elapsed":3,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# Data Load and Target Calculation"],"metadata":{"id":"mbdoxohfqVee"}},{"cell_type":"code","source":["# load the shortage data\n","shortage_df = spark.read.parquet(f\"{DATA_PATH}/preprocessed/feng_spark.parquet\")\n","\n","def build_shortage_flag(df, weeks_ahead, cv_end_date):\n","    # build the target variable\n","    df = (\n","        df.withColumn(\n","            \"shortage_flag\",\n","            F.col(\"shortage_indicator\")\n","            - F.lag(F.col(\"shortage_indicator\"), 1).over(\n","                Window.partitionBy(\"package_ndc\").orderBy(\"cd_day\")\n","            )\n","        )\n","        .withColumn(\n","            \"shortage_flag\",\n","            F.max(F.col(\"shortage_flag\"))\n","            .over(\n","                Window.partitionBy(\"package_ndc\")\n","                .orderBy(\"cd_day\")\n","                .rowsBetween(-(weeks_ahead - 1), 0)\n","            )\n","        )\n","        .withColumn(\n","            \"shortage_flag\",\n","            F.lag(F.col(\"shortage_flag\"), -weeks_ahead).over(\n","                Window.partitionBy(\"package_ndc\").orderBy(\"cd_day\")\n","            ),\n","        )\n","        .filter(F.col(\"shortage_flag\").isNotNull())\n","        .repartition(\"cd_day\")\n","        .cache()\n","    )\n","\n","    # save the test data\n","    train_data, test_data = adjust_train_and_test(\n","        df.filter(\n","            (F.col(\"cd_day\") < int(cv_end_date))\n","            & (F.col(\"cd_day\") >= int(MIN_DATE))\n","        ),\n","        df.filter(\n","            (F.col(\"cd_day\") >= int(cv_end_date))\n","            & (F.col(\"cd_day\") < int(END_TEST_DATE))\n","        )\n","    )\n","    train_data = train_data.cache()\n","    test_data = test_data.cache()\n","\n","    return train_data, test_data"],"metadata":{"id":"ITYGZj9aXYR3","executionInfo":{"status":"ok","timestamp":1710795341869,"user_tz":180,"elapsed":6861,"user":{"displayName":"Pedro Forli","userId":"08325002347257917995"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# Experiment"],"metadata":{"id":"BgUmWyOeqS6I"}},{"cell_type":"markdown","source":["## Cross-Validation"],"metadata":{"id":"wsqYAizhBW8v"}},{"cell_type":"code","source":["for weeks_ahead in range(10, 11):\n","    cvStartDate = (\n","        datetime.strptime(START_CV_DATE, \"%Y%m%d\")\n","        - relativedelta(weeks=weeks_ahead - WEEKS_AHEAD)\n","    ).strftime(\"%Y%m%d\")\n","    cvEndDate = (\n","        datetime.strptime(END_CV_DATE, \"%Y%m%d\")\n","        - relativedelta(weeks=weeks_ahead - WEEKS_AHEAD)\n","    ).strftime(\"%Y%m%d\")\n","\n","    print(\"WEEKS AHEAD =\", weeks_ahead)\n","    print(cvStartDate, cvEndDate)\n","\n","    train_data, test_data = build_shortage_flag(shortage_df, weeks_ahead, cvEndDate)\n","\n","    # build the transformer pipeline\n","    transform_stages = build_transformation_pipeline(\n","        numerical_features=NUMERICAL_FEATURES,\n","        norm_features=NORM_FEATURES,\n","        sindex_cols=STRING_INDEX,\n","        ohe_cols=OHE_FEATURES,\n","    )\n","\n","    # set the appropriate experiment\n","    exp_name = f\"{EXPERIMENT_TAG}_{DATASET_TARGET.replace('3M', '03M')}\"\n","    try:\n","        exp_id = mlflow.create_experiment(exp_name, artifact_location=f\"{PROJECT_PATH}/mlflow/content/mlruns\")\n","    except:\n","        exp_id = mlflow.set_experiment(exp_name).experiment_id\n","\n","    # run the training loop\n","    for alg_object in ALGORITHMS:\n","\n","        # instantiate the object\n","        alg = alg_object()\n","        pipeline = Pipeline(stages=transform_stages + [alg])\n","\n","        # Set Objective Functions\n","        def objective_function(space):\n","            # Log Evaluation Number\n","            with mlflow.start_run(nested=True):\n","                mlflow.log_metric(\"Evaluation Number\", objective_function.evaluation_number)\n","                objective_function.evaluation_number += 1\n","\n","                # build the parameter grid\n","                paramGrid = ParamGridBuilder()\n","                for p, v in space.items():\n","                    paramGrid.addGrid(alg.getParam(p), [v])\n","                paramGrid = paramGrid.build()\n","\n","                # setup the cross validation object\n","                crossval = TimeSeriesCrossValidator(\n","                    estimator=pipeline,\n","                    estimatorParamMaps=paramGrid,\n","                    evaluator=BinaryClassificationEvaluator(labelCol=\"shortage_flag\"),\n","                    dateCol=\"cd_day\",\n","                    cvStartDate=cvStartDate,\n","                    cvEndDate=cvEndDate,\n","                    cvValSize=CV_SET_SIZE,\n","                    cvValStep=CV_SET_STEP,\n","                    numFolds=NUM_FOLDS,\n","                )\n","\n","                # fit the model\n","                for i in range(3):\n","                  try:\n","                      cvModel = crossval.fit(train_data)\n","                  except:\n","                      continue\n","                  else:\n","                      # Since objective function passes a single hyperparameter combination, TimeSeriesCrossValidator will only have 1\n","                      # value in cvModel.avgMetrics.\n","                      avg_metrics = cvModel.avgMetrics[0]\n","\n","                      # log the final AUC performance\n","                      mlflow.log_metric(\"avg_areaUnderROC\", avg_metrics)\n","\n","                      # Optimizes lower is better. Setting to negative treats metric as a loss function\n","                      return -avg_metrics\n","\n","                return 0\n","\n","        objective_function.evaluation_number = 1\n","        rid = pd.to_datetime(\"today\").strftime(\"%Y%m%d%H%M%S\")\n","        with mlflow.start_run(run_name=rid, experiment_id=exp_id):\n","            # hyperparameter tuning\n","            trials = Trials()\n","            best_hyperparam_indices = fmin(\n","                objective_function,\n","                space=GRID_SPACE[alg_object],\n","                algo=tpe.suggest,\n","                trials=trials,\n","                max_evals=NUM_EVALS,\n","                rstate=np.random.default_rng(42),\n","            )\n","            # fmin returns the index of the best choice for hp.choice param options but returns the actual value of all other\n","            # param grid suggestions. space_eval evaluates the hp.choice options to return the value\n","            best_hyperparams = space_eval(GRID_SPACE[alg_object], best_hyperparam_indices)\n","\n","            # log the parameters of the execution\n","            params = {\n","                \"Algorithm Used\": str(alg_object).split(\".\")[-1].replace(\"'>\", \"\"),\n","                \"Features 1 - All\": FEATURES,\n","                \"Features 2 - Numerical\": NUMERICAL_FEATURES,\n","                \"Features 3 - Normalized\": NORM_FEATURES,\n","                \"Features 4 - String Indexed\": STRING_INDEX,\n","                \"Features 5 - OHE\": OHE_FEATURES,\n","            }\n","            for p, v in params.items():\n","                mlflow.log_param(p, v)\n","\n","            # log results of best model\n","            best_alg = alg_object(**best_hyperparams)\n","            best_pipeline = Pipeline(stages=transform_stages + [best_alg])\n","\n","            best_model = best_pipeline.fit(train_data)\n","\n","            # get the test results\n","            test_results = get_all_metrics(best_model, test_data)\n","            for m, v in test_results.items():\n","                mlflow.log_metric(f\"test_{m}\", v)\n","\n","            mlflow.spark.log_model(best_model, \"model\")\n","\n","    print(\"WEEKS AHEAD =\", weeks_ahead)\n","    print(\"TEST RESULTS:\")\n","    print(test_results)\n","    print(\"BEST HYPERPARAMETERS:\")\n","    print(best_hyperparams)"],"metadata":{"id":"kafJ8DI-qSis","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fa0e3aea-c9cb-48b6-942c-9ab9c41b413c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WEEKS AHEAD = 10\n","20221221 20230620\n"," 87%|████████▋ | 13/15 [3:52:32<43:57, 1319.00s/trial, best loss: -0.9342868071045042]"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"itygHkuURVev"}}]}